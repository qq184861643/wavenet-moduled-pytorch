{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "from torchaudio import transforms\n",
    "from wavenet import WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VCTK(Dataset):\n",
    "    def __init__(self,path='./VCTK/',speaker='p225',transform=None,sr=16000,top_db=10):\n",
    "        self.wav_list = glob.glob(path + speaker +'/*.wav')\n",
    "        self.wav_ids = sorted([f.split('/')[-1] for f in glob.glob(path+'*')])\n",
    "        self.transform = transform\n",
    "        self.sr = sr\n",
    "        self.top_db = top_db\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        f = self.wav_list[index]\n",
    "        audio,_ = librosa.load(f,sr=self.sr,mono=True)\n",
    "        audio,_ = librosa.effects.trim(audio, top_db=self.top_db, frame_length=2048)\n",
    "        audio = np.clip(audio,-1,1)\n",
    "        wav_tensor = torch.from_numpy(audio).unsqueeze(1)\n",
    "        wav_id = f.split('/')[3]\n",
    "        if self.transform is not None:\n",
    "            wav_tensor = self.transform(wav_tensor)\n",
    "        \n",
    "        return wav_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wav_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = transforms.Compose([\n",
    "        transforms.MuLawEncoding(),\n",
    "        transforms.LC2CL()])\n",
    "\n",
    "def collate_fn_(batch_data, max_len=40000):\n",
    "    audio = batch_data[0]\n",
    "    audio_len = audio.size(1)\n",
    "    if audio_len > max_len:\n",
    "        idx = random.randint(0,audio_len - max_len)\n",
    "        return audio[:,idx:idx+max_len]\n",
    "    else:\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vctk = VCTK(speaker='p225',transform=t,sr=16000)\n",
    "training_data = DataLoader(vctk,batch_size=1, shuffle=True,collate_fn=collate_fn_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = WaveNet().cuda()\n",
    "train_step = optim.Adam(model.parameters(),lr=2e-2, eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.MultiStepLR(train_step, milestones=[50,150,250], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.54576349258\n",
      "5 5.17097759247\n",
      "10 4.68942022324\n",
      "15 4.11701393127\n",
      "20 3.97902035713\n",
      "25 2.99311327934\n",
      "30 2.1784787178\n",
      "35 1.17133200169\n",
      "40 0.672031521797\n",
      "45 0.0956522896886\n",
      "50 0.0156723242253\n",
      "55 0.00487605528906\n",
      "60 0.00221115117893\n",
      "65 0.00123464234639\n",
      "70 0.000777395616751\n",
      "75 0.000537805375643\n",
      "80 0.000402514386224\n",
      "85 0.000320124527207\n",
      "90 0.000266821269179\n",
      "95 0.000230616744375\n",
      "100 0.0002049238974\n",
      "105 0.000185854369192\n",
      "110 0.000171155334101\n",
      "115 0.000159416595125\n",
      "120 0.000149726387463\n",
      "125 0.000141529060784\n",
      "130 0.000134454618092\n",
      "135 0.000128221974592\n",
      "140 0.00012267632701\n",
      "145 0.000117670024338\n",
      "150 0.000113103778858\n",
      "155 0.000110973996925\n",
      "160 0.000108949359856\n",
      "165 0.000107010884676\n",
      "170 0.000105135623016\n",
      "175 0.000103312275314\n",
      "180 0.000101556099253\n",
      "185 9.9858691101e-05\n",
      "190 9.81919220067e-05\n",
      "195 9.65765866567e-05\n",
      "200 9.49896129896e-05\n",
      "205 9.34484269237e-05\n",
      "210 9.19663652894e-05\n",
      "215 9.04959524632e-05\n",
      "220 8.90813025762e-05\n",
      "225 8.76989797689e-05\n",
      "230 8.63427339937e-05\n",
      "235 8.50333526614e-05\n",
      "240 8.37433253764e-05\n",
      "245 8.2488739281e-05\n",
      "250 8.12577127363e-05\n",
      "255 8.06585449027e-05\n",
      "260 8.00574489404e-05\n",
      "265 7.94908482931e-05\n",
      "270 7.88951583672e-05\n",
      "275 7.83316791058e-05\n",
      "280 7.77663954068e-05\n",
      "285 7.71863342379e-05\n",
      "290 7.662706048e-05\n",
      "295 7.60759576224e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    loss_= []\n",
    "    scheduler.step()\n",
    "    for data in training_data:\n",
    "        \n",
    "        data = Variable(data).cuda()\n",
    "        x = data[:,:-1]\n",
    "        logits = model(x)\n",
    "        y = data[:,-logits.size(2):]\n",
    "        loss = F.cross_entropy(logits.transpose(1,2).contiguous().view(-1,256), y.view(-1))\n",
    "        train_step.zero_grad()\n",
    "        loss.backward()\n",
    "        train_step.step()\n",
    "        loss_.append(loss.data[0])\n",
    "    if (epoch+1)%20 == 0:\n",
    "        torch.save(model.state_dict(),'model_%s.pth'%(str(epoch+1)))\n",
    "    if epoch%5 == 0:\n",
    "        print epoch,np.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       " -3.2650e+01 -3.1655e+01 -3.4533e+01  ...  -4.2706e+01 -4.0256e+01 -3.6308e+01\n",
       " -3.1576e+01 -3.0497e+01 -3.8252e+01  ...  -4.3902e+01 -3.7529e+01 -3.8794e+01\n",
       " -3.3516e+01 -3.3208e+01 -3.5913e+01  ...  -3.9251e+01 -4.2566e+01 -3.7099e+01\n",
       "                 ...                   â‹±                   ...                \n",
       " -3.2975e+01 -3.3029e+01 -3.4870e+01  ...  -4.1254e+01 -3.7543e+01 -3.8281e+01\n",
       " -3.3611e+01 -3.5202e+01 -3.5310e+01  ...  -4.1819e+01 -4.1373e+01 -3.6366e+01\n",
       " -3.0597e+01 -3.0185e+01 -3.1446e+01  ...  -4.1941e+01 -4.1382e+01 -3.5701e+01\n",
       "[torch.cuda.FloatTensor of size 1x256x15872 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "audio,_ = librosa.load('./VCTK/p225/p225_001.wav',sr=16000,mono=True)\n",
    "audio,_ = librosa.effects.trim(audio, top_db=10, frame_length=2048)\n",
    "wav_tensor = torch.from_numpy(audio).unsqueeze(1)\n",
    "wav_tensor = transforms.MuLawEncoding()(wav_tensor).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06f0e7957e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecp_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5116\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrecp_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrecp_field\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "recp_field=5116\n",
    "sample_len = 16000*3\n",
    "sample = Variable(wav_tensor[:,:recp_field]).cuda()\n",
    "for i in range(sample_len):\n",
    "    logits = model(sample[:,-recp_field:])\n",
    "    m = torch.distributions.Categorical(F.softmax(logits,dim=1).view(-1))\n",
    "    new = m.sample().view(1,-1)\n",
    "    #print sample.size(),new.size()\n",
    "    sample = torch.cat((sample,new),dim=1)\n",
    "    print sample.size()\n",
    "    if i % 16000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5117])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
